{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from agentsClasses.OfflineDQNClass import OfflineDQNAgent, OfflineDDQNAgent\n",
    "from utilities.plots import create_grids, plot_trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "show_stats = True       #Mostrar stats de los agentes al terminar el entrenamiento\n",
    "render = True           #Mostrar el grid con la política y trayectoria obtenidas\n",
    "\n",
    "NUM_DQN_AGENTS = 1      #Numero de agentes DQN a entrenar\n",
    "NUM_DDQN_AGENTS = 1     #Numero de agentes DDQN a entrenar\n",
    "\n",
    "#Estructura de las redes neuronales de los agentes\n",
    "NUM_NEURONS_FC1 = 128   #Neuronas de la primera capa fully connected\n",
    "NUM_NEURONS_FC2 = 128   #Neuronas de la segunda primera capa fully connected\n",
    "\n",
    "SHAPE = \"5x5\"           #Tamanho del grid \"5x5\" or \"14x14\"\n",
    "\n",
    "if SHAPE == \"5x5\":\n",
    "    EPISODES_PER_AGENT = 1000       #Episodios por agente\n",
    "    MAX_STEPS_PER_EPISODE = 100     #Numero maximo de pasos por episodio\n",
    "    FOLDER_NAME = \"5x5_5_model\"     #Carpeta donde se encuentre el ensemble que se quiere usar\n",
    "    REWARD = [1000, -100, 0.3]      #[recompensa por alcanzar la meta, penalizacion por visitar estados por debajo de umbral de incertidumbre, umbral de incertidumbre para penalizar]\n",
    "\n",
    "elif SHAPE == \"14x14\":\n",
    "    EPISODES_PER_AGENT = 3000       #Episodios por agente\n",
    "    MAX_STEPS_PER_EPISODE = 300     #Numero maximo de pasos por episodio\n",
    "    FOLDER_NAME = \"5_model\"         #Carpeta donde se encuentre el ensemble que se quiere usar\n",
    "    REWARD = [1000, -1, 0.3]        #[recompensa por alcanzar la meta, penalizacion por visitar estados por debajo de umbral de incertidumbre, umbral de incertidumbre para penalizar]\n",
    "\n",
    "#Parametros de entrenamiento\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = 0.9996      #En este caso la caida de epsilon es fija y no depende del numero de episodios\n",
    "\n",
    "BUFFER_SIZE = int(1e5)  #Tamaño del buffer de repeticion de experiencias\n",
    "BATCH_SIZE = 64         #Tamaño de minibatch\n",
    "GAMMA = 0.99            #Factor de descuento\n",
    "TAU = 1e-3              #Para realizar el soft update de los parametros target\n",
    "LR = 5e-4               #Learning rate\n",
    "UPDATE_EVERY = 4        #Cada cuantos pasos actualizar la red Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATING AND TRAINING THE AGENTS\n",
    "agents_arr = []         # array of agents\n",
    "\n",
    "print(\"Starting training of\", NUM_DQN_AGENTS, \"DQN agents and\", NUM_DDQN_AGENTS, \"DDQN agents\")\n",
    "\n",
    "for i in range(NUM_DQN_AGENTS+NUM_DDQN_AGENTS):\n",
    "    if i < NUM_DQN_AGENTS:\n",
    "        print(\"DQN Agent\", i+1,\"/\",NUM_DQN_AGENTS)    # if the agent is a DQN agent\n",
    "        agent = OfflineDQNAgent(i, \"DQN\", SHAPE, FOLDER_NAME, NUM_NEURONS_FC1, NUM_NEURONS_FC2, EPISODES_PER_AGENT, MAX_STEPS_PER_EPISODE, REWARD, EPS_START, EPS_END, EPS_DECAY, BUFFER_SIZE, BATCH_SIZE, GAMMA, TAU, LR, UPDATE_EVERY)\n",
    "    else:\n",
    "        print(\"DDQN Agent\", i+1-NUM_DQN_AGENTS,\"/\",NUM_DDQN_AGENTS)  # if the agent is a DDQN agent \n",
    "        agent = OfflineDDQNAgent(i, \"DDQN\", SHAPE, FOLDER_NAME, NUM_NEURONS_FC1, NUM_NEURONS_FC2, EPISODES_PER_AGENT, MAX_STEPS_PER_EPISODE, REWARD, EPS_START, EPS_END, EPS_DECAY, BUFFER_SIZE, BATCH_SIZE, GAMMA, TAU, LR, UPDATE_EVERY)\n",
    "\n",
    "    agent.train()\n",
    "    agents_arr.append(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_stats:\n",
    "    for agent in agents_arr:\n",
    "        print(\"La longitud estimada para el eje X del training error es:\", (agent.total_steps/UPDATE_EVERY)*BATCH_SIZE)\n",
    "        agent.plot_results(rolling_length=5, rolling_error=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent in agents_arr:\n",
    "    value_grid, policy_grid, string_policy_grid = create_grids(agent.env, Qnet=agent.qnetwork_local)\n",
    "    start_pos = agent.env.unwrapped.start_pos\n",
    "\n",
    "    if render:\n",
    "        plot_trajectory(string_policy_grid, start_pos, agent.id+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hay muchos elementos en el rewardHistory que son iguales, quiero que solo haya uno de cada\n",
    "#historico = dict.fromkeys(agent.env.unwrapped.rewardHistory)\n",
    "\n",
    "print(len(agent.env.unwrapped.rewardHistory))\n",
    "\n",
    "unicos = []\n",
    "\n",
    "for i in agent.env.unwrapped.rewardHistory:\n",
    "\n",
    "    stringo = str(i[0][0]) + ',' + str(i[0][1]) + ',' + str(i[1]) + ',' + str(i[2][0]) + ',' + str(i[2][1])\n",
    "    \n",
    "    if stringo not in unicos:\n",
    "        unicos.append(stringo)\n",
    "\n",
    "for stringo in unicos:\n",
    "    print(stringo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
