{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import sys\n",
        "sys.path.append('../')\n",
        "\n",
        "from agentsClasses.OfflineQLearningClass import OfflineQLearningAgent\n",
        "from utilities.plots import create_grids, plot_trajectory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "#Hyperparameters\n",
        "show_stats = True       #Mostrar stats de los agentes al terminar el entrenamiento\n",
        "render = True           #Mostrar el grid con la política y trayectoria obtenidas\n",
        "\n",
        "NUM_AGENTS = 1          #Numero de agentes a entrenar\n",
        "\n",
        "SHAPE = \"14x14\"         #Tamanho del grid \"5x5\" or \"14x14\"\n",
        "\n",
        "if SHAPE == \"5x5\":\n",
        "    EPISODES_PER_AGENT = 1000       #Episodios por agente\n",
        "    MAX_STEPS_PER_EPISODE = 100     #Numero maximo de pasos por episodio\n",
        "    FOLDER_NAME = \"5x5_5_model\"     #Carpeta donde se encuentre el ensemble que se quiere usar\n",
        "elif SHAPE == \"14x14\":\n",
        "    EPISODES_PER_AGENT = 3000       #Episodios por agente\n",
        "    MAX_STEPS_PER_EPISODE = 300     #Numero maximo de pasos por episodio\n",
        "    FOLDER_NAME = \"5_model\"         #Carpeta donde se encuentre el ensemble que se quiere usar\n",
        "\n",
        "REWARD = [1000, -100, 0.3]     #[recompensa por alcanzar la meta, penalizacion por visitar estados por debajo de umbral de incertidumbre, umbral de incertidumbre para penalizar]\n",
        "\n",
        "#Parametros de entrenamiento\n",
        "EPS_START = 1.0\n",
        "EPS_DECAY = EPS_START/(EPISODES_PER_AGENT/2) #Se reduce la exploracion a medida que avanza el experimento\n",
        "EPS_END = 0.1\n",
        "\n",
        "DISCOUNT_FACTOR = 0.95  #Factor de descuento\n",
        "LR = 0.02               #Learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "#CREATING AND TRAINING THE AGENTS\n",
        "agents_arr = []\n",
        "\n",
        "print(\"Starting training of\", NUM_AGENTS, \"Q-learning agents\")\n",
        "\n",
        "for i in range(NUM_AGENTS):\n",
        "    print(\"Agent\", i+1, \"/\", NUM_AGENTS)\n",
        "    agent = OfflineQLearningAgent(i, SHAPE, FOLDER_NAME, EPISODES_PER_AGENT, MAX_STEPS_PER_EPISODE, REWARD, LR, EPS_START, EPS_DECAY, EPS_END, DISCOUNT_FACTOR)\n",
        "    \n",
        "    agent.train()\n",
        "    agents_arr.append(agent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if show_stats:\n",
        "    for agent in agents_arr:\n",
        "        print(\"La longitud estimada para el eje X del training error es:\", agent.total_steps)\n",
        "        agent.plot_results(rolling_length=1, rolling_error=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for agent in agents_arr:\n",
        "    value_grid, policy_grid, string_policy_grid = create_grids(agent.env, agent=agent)\n",
        "    start_pos = agent.env.unwrapped.start_pos\n",
        "\n",
        "    if render:\n",
        "        plot_trajectory(string_policy_grid, start_pos, agent.id+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Hay muchos elementos en el rewardHistory que son iguales. Con esta función se eliminan las repeticiones.\n",
        "#De esta forma se puede ver que transiciones supusieron una penalizacion.\n",
        "#historico = dict.fromkeys(agent.env.unwrapped.rewardHistory)\n",
        "\n",
        "print(len(agent.env.unwrapped.rewardHistory))\n",
        "\n",
        "unicos = []\n",
        "\n",
        "for i in agent.env.unwrapped.rewardHistory:\n",
        "\n",
        "    stringo = str(i[0][0]) + ',' + str(i[0][1]) + ',' + str(i[1]) + ',' + str(i[2][0]) + ',' + str(i[2][1])\n",
        "    \n",
        "    if stringo not in unicos:\n",
        "        unicos.append(stringo)\n",
        "\n",
        "for stringo in unicos:\n",
        "    print(stringo)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
