{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from classes.DQNagentClass import DQNAgent, DDQNAgent\n",
    "from envs.createEnvs import createNNEnv\n",
    "from utilities.plots import create_dicts_DQN, create_grids, plot_trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n",
      "Models loaded\n"
     ]
    }
   ],
   "source": [
    "shape=\"5x5\"\n",
    "\n",
    "env = createNNEnv(shape)#, disable_env_checker=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAgent(agent, path_to_save, n_episodes = 1000, max_t = 3000, eps_start = 1.0, eps_end = 0.01, eps_decay = 0.996):\n",
    "    \"\"\"Deep Q-Learning\n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training epsiodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon \n",
    "        eps_decay (float): mutiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = [] # list containing score from each episode\n",
    "    durations = [] # list containing duration of each episode\n",
    "    eps = eps_start\n",
    "\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state,_ = env.reset()\n",
    "        score = 0\n",
    "        for t in range(1, max_t+1):\n",
    "            action = agent.act(state,eps)\n",
    "            #print(\"State: \", state,\"Action: \",action)\n",
    "            next_state,reward,done,_,_ = env.step(action)\n",
    "            agent.step(state,action,reward,next_state,done)\n",
    "            ## above step decides whether we will train(learn) the network actor (local_qnetwork) or we will fill the replay buffer\n",
    "            ## if len replay buffer is equal to the batch size then we will train the network or otherwise we will add experience tuple in our replay buffer.\n",
    "            state = next_state\n",
    "            score += reward\n",
    "\n",
    "            if done or t == max_t:\n",
    "                print('Episode: {}\\tSteps: {}'.format(i_episode,t))\n",
    "                scores.append(score)\n",
    "                durations.append(t)\n",
    "                break\n",
    "\n",
    "            eps = max(eps*eps_decay,eps_end)## decrease the epsilon\n",
    "\n",
    "    # save the model weights\n",
    "    torch.save(agent.qnetwork_local.state_dict(), path_to_save)\n",
    "\n",
    "    return scores, durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#print(device)\n",
    "#print(env.action_space.n, env.observation_space.shape[0])\n",
    "\n",
    "#Si cambio los seed de los agentes superan muchas veces el max_t de 200\n",
    "\"\"\"dqnagent1 = DQNAgent(state_size=env.observation_space.shape[0], action_size=env.action_space.n, seed=0, fc1_unit=64, fc2_unit=64)\n",
    "dqnagent2 = DQNAgent(state_size=env.observation_space.shape[0], action_size=env.action_space.n, seed=0, fc1_unit=64, fc2_unit=128)\n",
    "dqnagent3 = DQNAgent(state_size=env.observation_space.shape[0], action_size=env.action_space.n, seed=0, fc1_unit=128, fc2_unit=64)\n",
    "dqnagent4 = DQNAgent(state_size=env.observation_space.shape[0], action_size=env.action_space.n, seed=0, fc1_unit=128, fc2_unit=128)\n",
    "dqnagent5 = DQNAgent(state_size=env.observation_space.shape[0], action_size=env.action_space.n, seed=0, fc1_unit=128, fc2_unit=128)\n",
    "\n",
    "ddqnagent1 = DDQNAgent(state_size=env.observation_space.shape[0], action_size=env.action_space.n, seed=0, fc1_unit=64, fc2_unit=64)\n",
    "ddqnagent2 = DDQNAgent(state_size=env.observation_space.shape[0], action_size=env.action_space.n, seed=0, fc1_unit=64, fc2_unit=128)\n",
    "ddqnagent3 = DDQNAgent(state_size=env.observation_space.shape[0], action_size=env.action_space.n, seed=0, fc1_unit=128, fc2_unit=64)\n",
    "ddqnagent4 = DDQNAgent(state_size=env.observation_space.shape[0], action_size=env.action_space.n, seed=0, fc1_unit=128, fc2_unit=128)\n",
    "ddqnagent5 = DDQNAgent(state_size=env.observation_space.shape[0], action_size=env.action_space.n, seed=0, fc1_unit=128, fc2_unit=128)\"\"\"\n",
    "\n",
    "dqnagent1 = DQNAgent(state_size=env.observation_space.shape[0], action_size=env.action_space.n, seed=0, fc1_unit=128, fc2_unit=128)\n",
    "dqnagent2 = DQNAgent(state_size=env.observation_space.shape[0], action_size=env.action_space.n, seed=0, fc1_unit=128, fc2_unit=128)\n",
    "dqnagent3 = DQNAgent(state_size=env.observation_space.shape[0], action_size=env.action_space.n, seed=0, fc1_unit=128, fc2_unit=128)\n",
    "dqnagent4 = DQNAgent(state_size=env.observation_space.shape[0], action_size=env.action_space.n, seed=0, fc1_unit=128, fc2_unit=128)\n",
    "dqnagent5 = DQNAgent(state_size=env.observation_space.shape[0], action_size=env.action_space.n, seed=0, fc1_unit=128, fc2_unit=128)\n",
    "\n",
    "ddqnagent1 = DDQNAgent(state_size=env.observation_space.shape[0], action_size=env.action_space.n, seed=0, fc1_unit=128, fc2_unit=128)\n",
    "ddqnagent2 = DDQNAgent(state_size=env.observation_space.shape[0], action_size=env.action_space.n, seed=0, fc1_unit=128, fc2_unit=128)\n",
    "ddqnagent3 = DDQNAgent(state_size=env.observation_space.shape[0], action_size=env.action_space.n, seed=0, fc1_unit=128, fc2_unit=128)\n",
    "ddqnagent4 = DDQNAgent(state_size=env.observation_space.shape[0], action_size=env.action_space.n, seed=0, fc1_unit=128, fc2_unit=128)\n",
    "ddqnagent5 = DDQNAgent(state_size=env.observation_space.shape[0], action_size=env.action_space.n, seed=0, fc1_unit=128, fc2_unit=128)\n",
    "\n",
    "\n",
    "agents_arr = [dqnagent1, dqnagent2, dqnagent3, dqnagent4, dqnagent5, ddqnagent1, ddqnagent2, ddqnagent3, ddqnagent4, ddqnagent5]\n",
    "scores_arr = []\n",
    "durations_arr = []\n",
    "starting_positions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n",
      "Models loaded\n",
      "DQN Agent 1 / 10\n",
      "Loading models...\n",
      "Models loaded\n",
      "DQN Agent 2 / 10\n",
      "Loading models...\n",
      "Models loaded\n",
      "DQN Agent 3 / 10\n",
      "Loading models...\n",
      "Models loaded\n",
      "DQN Agent 4 / 10\n",
      "Loading models...\n",
      "Models loaded\n",
      "DQN Agent 5 / 10\n",
      "Loading models...\n",
      "Models loaded\n",
      "DDQN Agent 6 / 10\n",
      "Loading models...\n",
      "Models loaded\n",
      "DDQN Agent 7 / 10\n",
      "Loading models...\n",
      "Models loaded\n",
      "DDQN Agent 8 / 10\n",
      "Loading models...\n",
      "Models loaded\n",
      "DDQN Agent 9 / 10\n",
      "Loading models...\n",
      "Models loaded\n",
      "DDQN Agent 10 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\el_sa\\anaconda3\\envs\\TFG\\Lib\\site-packages\\gymnasium\\envs\\registration.py:694: UserWarning: \u001b[33mWARN: Overriding environment gridNN-v0 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    }
   ],
   "source": [
    "#TRAINING\n",
    "train = True\n",
    "NUM_DQN_AGENTS = 5\n",
    "NUM_DDQN_AGENTS = 5\n",
    "\n",
    "if train:\n",
    "    for agent in agents_arr:\n",
    "        env.unwrapped.randomize_start_pos()\n",
    "        if agents_arr.index(agent) < NUM_DQN_AGENTS:\n",
    "            print(\"DQN Agent\", agents_arr.index(agent)+1,\"/\",len(agents_arr))\n",
    "            path_to_save = \"../data/models/DQNagent\"+str(agents_arr.index(agent)+1)+\".pt\"\n",
    "        else:\n",
    "            print(\"DDQN Agent\", agents_arr.index(agent)+1,\"/\",len(agents_arr))\n",
    "            path_to_save = \"../data/models/DDQNagent\"+str(agents_arr.index(agent)+1-NUM_DQN_AGENTS)+\".pt\"\n",
    "        scores, durations = trainAgent(agent, path_to_save)\n",
    "        scores_arr.append(scores)\n",
    "        durations_arr.append(durations)\n",
    "        starting_positions.append(env.unwrapped.start_pos)\n",
    "\n",
    "\n",
    "else:\n",
    "    for agent in agents_arr:\n",
    "        #Aqui tenemos el problema de no conocer sus starting positions originales??\n",
    "        env = createNNEnv(shape, randomStart=True)\n",
    "        if agents_arr.index(agent) < NUM_DQN_AGENTS:\n",
    "            print(\"DQN Agent\", agents_arr.index(agent)+1,\"/\",len(agents_arr))\n",
    "            path_to_load = \"../data/models/DQNagent\"+str(agents_arr.index(agent)+1)+\".pt\"\n",
    "        else:\n",
    "            print(\"DDQN Agent\", agents_arr.index(agent)+1,\"/\",len(agents_arr))\n",
    "            path_to_load = \"../data/models/DDQNagent\"+str(agents_arr.index(agent)+1-NUM_DQN_AGENTS)+\".pt\"\n",
    "        agent.qnetwork_local.load_state_dict(torch.load(path_to_load))\n",
    "        agent.qnetwork_local.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si no entreno, no puedo plotear pq desconozco los scores y las duraciones\n",
    "if train:\n",
    "    # Create a figure with subplots for each pair\n",
    "    fig, axes = plt.subplots(len(agents_arr), 2, figsize=(12, 5*len(agents_arr)))\n",
    "\n",
    "    for i in range(len(agents_arr)):\n",
    "        scores = scores_arr[i]  # Access the scores for the current agent\n",
    "        durations = durations_arr[i]  # Access the durations for the current agent\n",
    "\n",
    "        # Print information before each pair of subplots\n",
    "        print(\"Agent\", i+1, \"steps stats:\", \"\\tAverage\", round(np.mean(durations), 2), \"\\tStd dev\", round(np.std(durations), 2), \"\\tMedian\", round(np.median(durations), 2))\n",
    "        ######print(\"fc1_units:\", agents_arr[i].qnetwork_local.fc2.in_features, \"\\tfc2_units:\", agents_arr[i].qnetwork_local.fc2.out_features)\n",
    "        # Define the axes for the current agent's pair of subplots\n",
    "        ax_scores = axes[i, 0]\n",
    "        ax_durations = axes[i, 1]\n",
    "\n",
    "        # Plot the scores in the first subplot\n",
    "        ax_scores.plot(np.arange(len(scores)), scores)\n",
    "        ax_scores.set_ylabel('Score')\n",
    "        ax_scores.set_xlabel('Episode #')\n",
    "        ax_scores.set_title(f'Agent {i+1} - Scores')\n",
    "        \n",
    "        # Set Y-axis limits for scores between 0 and 1\n",
    "        ax_scores.set_ylim(0, 1.2)\n",
    "\n",
    "        # Plot the durations in the second subplot\n",
    "        ax_durations.plot(np.arange(len(durations)), durations, 'r')\n",
    "        ax_durations.set_ylabel('Steps')\n",
    "        ax_durations.set_xlabel('Episode #')\n",
    "        ax_durations.set_title(f'Agent {i+1} - Durations')\n",
    "\n",
    "        # Set Y-axis limits for durations between 0 and 1000\n",
    "        #####ax_durations.set_ylim(0, np.max(durations[i])+10)\n",
    "        ax_durations.set_ylim(0, 3000)\n",
    "\n",
    "    # Adjust layout for better readability\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the combined plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for i in range(len(agents_arr)):\\n    #print(\"Agent\", i+1, \"value grid:\")\\n    #print(value_grids[i])\\n    #print(\"Agent\", i+1, \"policy grid:\")\\n    #print(policy_grids[i])\\n    #print(\"Agent\", i+1, \"policy grid string:\")\\n    #print(string_policy_grids[i])\\n    fig = plot_trajectory(string_policy_grids[i], starting_positions[i])\\n    fig.suptitle(f\\'Agent {i+1} - Policy   Starting position: {starting_positions[i]}\\')\\nplt.show()'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"#value_grids = []\n",
    "#policy_grids = []\n",
    "string_policy_grids = []\n",
    "\n",
    "for agent in agents_arr:\n",
    "    state_value_dict, policy_dict = create_dicts_DQN(agent.qnetwork_local, env)\n",
    "    value_grid, policy_grid, string_policy_grid = create_grids(state_value_dict, policy_dict, env)\n",
    "    #value_grids.append(value_grid)\n",
    "    #policy_grids.append(policy_grid)\n",
    "    string_policy_grids.append(string_policy_grid)\"\"\"\n",
    "\n",
    "\"\"\"for i in range(len(agents_arr)):\n",
    "    #print(\"Agent\", i+1, \"value grid:\")\n",
    "    #print(value_grids[i])\n",
    "    #print(\"Agent\", i+1, \"policy grid:\")\n",
    "    #print(policy_grids[i])\n",
    "    #print(\"Agent\", i+1, \"policy grid string:\")\n",
    "    #print(string_policy_grids[i])\n",
    "    fig = plot_string_policy(string_policy_grids[i])\n",
    "    fig.suptitle(f'Agent {i+1} - Policy   Starting position: {starting_positions[i]}')\n",
    "plt.show()\"\"\"\n",
    "\n",
    "\"\"\"for i in range(len(agents_arr)):\n",
    "    #print(\"Agent\", i+1, \"value grid:\")\n",
    "    #print(value_grids[i])\n",
    "    #print(\"Agent\", i+1, \"policy grid:\")\n",
    "    #print(policy_grids[i])\n",
    "    #print(\"Agent\", i+1, \"policy grid string:\")\n",
    "    #print(string_policy_grids[i])\n",
    "    fig = plot_trajectory(string_policy_grids[i], starting_positions[i])\n",
    "    fig.suptitle(f'Agent {i+1} - Policy   Starting position: {starting_positions[i]}')\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    for i in range(len(agents_arr)):\n",
    "        state_value_dict, policy_dict = create_dicts_DQN(agents_arr[i].qnetwork_local, env)\n",
    "        value_grid, policy_grid, string_policy_grid = create_grids(state_value_dict, policy_dict, env)\n",
    "        fig = plot_trajectory(string_policy_grid, starting_positions[i])\n",
    "        fig.suptitle(f'Agent {i+1} - Policy   Starting position: {starting_positions[i]}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agente 1 terminado en 8 pasos\n"
     ]
    }
   ],
   "source": [
    "#Demostracion de un agente en el entorno\n",
    "\n",
    "#Hay que tener cuidado de darle correctamente el mismo tamaño de capas que al modelo que se quiere cargar\n",
    "agent_replay = DDQNAgent(state_size=env.observation_space.shape[0],action_size=env.action_space.n,seed=0, fc1_unit=128, fc2_unit=128)\n",
    "agent_replay.qnetwork_local.load_state_dict(torch.load('../data/models/DDQNagent4.pt'))\n",
    "agent_replay.qnetwork_local.eval()\n",
    "\n",
    "with open(f\"../data/csv/historyDQN.csv\", 'a') as f:\n",
    "    #f.write(f\"step,y,x,action,next_y,next_x,reward,done\\n\")\n",
    "    for i in range(1):\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "        t = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent_replay.act(obs)\n",
    "            prev_state = [obs[0], obs[1], action]\n",
    "            obs, rew, done, _, _ = env.step(action)\n",
    "\n",
    "            #f.write(f\"{t},{prev_state[0]},{prev_state[1]},{prev_state[2]},{obs[0]},{obs[1]},{rew},{done}\\n\")\n",
    "            t += 1\n",
    "        print(\"Agente\", i+1, \"terminado en\", t, \"pasos\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
