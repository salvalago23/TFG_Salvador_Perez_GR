{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from classes.DQNagentClass import DQNAgent, DDQNAgent\n",
    "from envs.createEnvs import createNNEnv\n",
    "from utilities.plots import create_grids, plot_trajectory\n",
    "from utilities.jsonRW import writeJSON, readJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n",
      "Models loaded\n"
     ]
    }
   ],
   "source": [
    "#CREATING THE ENVIRONMENT\n",
    "shape = \"5x5\"             # \"5x5\" or \"14x14\"\n",
    "env = createNNEnv(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "train = True            # train or test\n",
    "show_stats = False       # show stats\n",
    "export_to_JSON = True   # write JSON file\n",
    "render = False           # render the results after training\n",
    "\n",
    "NUM_DQN_AGENTS = 10     # number of DQN agents\n",
    "NUM_DDQN_AGENTS = 10     # number of DDQN agents\n",
    "\n",
    "SEED = 0                # random seed. 0 for all\n",
    "NUM_NEURONS_FC1 = 128   # number of neurons for the first fully connected layer\n",
    "NUM_NEURONS_FC2 = 128   # number of neurons for the second fully connected layer\n",
    "\n",
    "EPISODES_PER_AGENT = 2000\n",
    "MAX_STEPS_PER_EPISODE = 25\n",
    "\n",
    "EPS_START = 1.0         # epsilon start value\n",
    "EPS_END = 0.01          # epsilon end value\n",
    "EPS_DECAY = 0.996       # epsilon decay rate\n",
    "#EPS_DECAY = EPS_START/(EPISODES_PER_AGENT/2)\n",
    "\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 5e-4               # learning rate\n",
    "UPDATE_EVERY = 4        # how often to update the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATING THE AGENTS\n",
    "agents_arr = []         # array of agents\n",
    "scores_arr = []         # array of scores of the episodes\n",
    "durations_arr = []      # array of durations of the episodes\n",
    "starting_positions = [] # array of starting positions for each agent\n",
    "\n",
    "for i in range(NUM_DQN_AGENTS):\n",
    "    path_to_save = \"../data/agent_models/pytorch/DQNagent\"+str(i+1)+\".pt\" # path to save the model\n",
    "    agent = DQNAgent(path_to_save, env, SEED, NUM_NEURONS_FC1, NUM_NEURONS_FC2, EPISODES_PER_AGENT, MAX_STEPS_PER_EPISODE, EPS_START, EPS_END, EPS_DECAY, BUFFER_SIZE, BATCH_SIZE, GAMMA, TAU, LR, UPDATE_EVERY)\n",
    "    agents_arr.append(agent) # append the agent to the array\n",
    "\n",
    "for i in range(NUM_DDQN_AGENTS):\n",
    "    path_to_save = \"../data/agent_models/pytorch/DDQNagent\"+str(i+1)+\".pt\" # path to save the model\n",
    "    agent = DDQNAgent(path_to_save, env, SEED, NUM_NEURONS_FC1, NUM_NEURONS_FC2, EPISODES_PER_AGENT, MAX_STEPS_PER_EPISODE, EPS_START, EPS_END, EPS_DECAY, BUFFER_SIZE, BATCH_SIZE, GAMMA, TAU, LR, UPDATE_EVERY)\n",
    "    agents_arr.append(agent) # append the agent to the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training of 10 DQN agents and 10 DDQN agents\n",
      "DQN Agent 1 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:29<00:00, 67.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN Agent 2 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:22<00:00, 87.04it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN Agent 3 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:29<00:00, 68.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN Agent 4 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:33<00:00, 59.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN Agent 5 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:22<00:00, 89.90it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN Agent 6 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:22<00:00, 88.03it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN Agent 7 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:38<00:00, 51.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN Agent 8 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:34<00:00, 57.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN Agent 9 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:39<00:00, 50.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN Agent 10 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:35<00:00, 56.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDQN Agent 1 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:27<00:00, 73.28it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDQN Agent 2 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:39<00:00, 51.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDQN Agent 3 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:23<00:00, 86.75it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDQN Agent 4 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:29<00:00, 68.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDQN Agent 5 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:29<00:00, 68.41it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDQN Agent 6 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:34<00:00, 58.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDQN Agent 7 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:32<00:00, 61.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDQN Agent 8 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:31<00:00, 62.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDQN Agent 9 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:22<00:00, 90.33it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDQN Agent 10 / 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:37<00:00, 53.90it/s]\n"
     ]
    }
   ],
   "source": [
    "#TRAINING\n",
    "if train:\n",
    "    print(\"Starting training of\", NUM_DQN_AGENTS, \"DQN agents and\", NUM_DDQN_AGENTS, \"DDQN agents\")\n",
    "    \n",
    "    for agent in agents_arr:\n",
    "        env.unwrapped.randomize_start_pos()     # randomize the starting position of the agent in the grid environment\n",
    "        if agents_arr.index(agent) < NUM_DQN_AGENTS:\n",
    "            print(\"DQN Agent\", agents_arr.index(agent)+1,\"/\",NUM_DQN_AGENTS)    # if the agent is a DQN agent\n",
    "        else:\n",
    "            print(\"DDQN Agent\", agents_arr.index(agent)+1-NUM_DQN_AGENTS,\"/\",NUM_DDQN_AGENTS)  # if the agent is a DDQN agent \n",
    "\n",
    "        scores, durations = agent.train()\n",
    "        scores_arr.append(scores)\n",
    "        durations_arr.append(durations)\n",
    "        starting_positions.append(env.unwrapped.start_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train and show_stats:\n",
    "    # Create a figure with subplots for each pair\n",
    "    fig, axes = plt.subplots(len(agents_arr), 2, figsize=(12, 5*len(agents_arr)))\n",
    "\n",
    "    for i in range(len(agents_arr)):\n",
    "        scores = scores_arr[i]  # Access the scores for the current agent\n",
    "        durations = durations_arr[i]  # Access the durations for the current agent\n",
    "\n",
    "        # Print information before each pair of subplots\n",
    "        print(\"Agent\", i+1, \"steps stats:\", \"\\tAverage\", round(np.mean(durations), 2), \"\\tStd dev\", round(np.std(durations), 2), \"\\tMedian\", round(np.median(durations), 2), \"\\tBest\", np.min(durations))\n",
    "        ######print(\"fc1_units:\", agents_arr[i].qnetwork_local.fc2.in_features, \"\\tfc2_units:\", agents_arr[i].qnetwork_local.fc2.out_features)\n",
    "        # Define the axes for the current agent's pair of subplots\n",
    "        ax_scores = axes[i, 0]\n",
    "        ax_durations = axes[i, 1]\n",
    "\n",
    "        # Plot the scores in the first subplot\n",
    "        ax_scores.plot(np.arange(len(scores)), scores)\n",
    "        ax_scores.set_ylabel('Score')\n",
    "        ax_scores.set_xlabel('Episode #')\n",
    "        ax_scores.set_title(f'Agent {i+1} - Scores')\n",
    "        \n",
    "        # Set Y-axis limits for scores between 0 and 1\n",
    "        ax_scores.set_ylim(0, 1.2)\n",
    "\n",
    "        # Plot the durations in the second subplot\n",
    "        ax_durations.plot(np.arange(len(durations)), durations, 'r')\n",
    "        ax_durations.set_ylabel('Steps')\n",
    "        ax_durations.set_xlabel('Episode #')\n",
    "        ax_durations.set_title(f'Agent {i+1} - Durations')\n",
    "\n",
    "        # Set Y-axis limits for durations \n",
    "        ax_durations.set_ylim(0, MAX_STEPS_PER_EPISODE+1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train:\n",
    "    for i in range(len(agents_arr)):\n",
    "        value_grid, policy_grid, string_policy_grid = create_grids(env, Qnet=agents_arr[i].qnetwork_local)\n",
    "        \n",
    "        if export_to_JSON:\n",
    "            if i < NUM_DQN_AGENTS:\n",
    "                algorithm = \"DQN\"\n",
    "            else:\n",
    "                algorithm = \"DDQN\"\n",
    "\n",
    "            writeJSON(algorithm, EPISODES_PER_AGENT, MAX_STEPS_PER_EPISODE, shape, starting_positions[i], value_grid, policy_grid, string_policy_grid)\n",
    "        \n",
    "        if render:\n",
    "            fig = plot_trajectory(string_policy_grid, starting_positions[i])\n",
    "            fig.suptitle(f'Agent {i+1} - Policy   Starting position: {starting_positions[i]}')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'agent_replay = DDQNAgent(state_size=env.observation_space.shape[0],action_size=env.action_space.n,seed=0, fc1_unit=128, fc2_unit=128)\\nagent_replay.qnetwork_local.load_state_dict(torch.load(\\'../data/agent_models/pytorch/DDQNagent4.pt\\'))\\nagent_replay.qnetwork_local.eval()\\n\\nwith open(f\"../data/csv/historyDQN.csv\", \\'a\\') as f:\\n    #f.write(f\"step,y,x,action,next_y,next_x,reward,done\\n\")\\n    for i in range(1):\\n        obs, _ = env.reset()\\n\\n        t = 0\\n        done = False\\n        while not done:\\n            action = agent_replay.act(obs)\\n            prev_state = [obs[0], obs[1], action]\\n            obs, rew, done, _, _ = env.step(action)\\n\\n            #f.write(f\"{t},{prev_state[0]},{prev_state[1]},{prev_state[2]},{obs[0]},{obs[1]},{rew},{done}\\n\")\\n            t += 1\\n        print(\"Agente\", i+1, \"terminado en\", t, \"pasos\")'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Para cargar todos los modelos que haya creados\n",
    "\"\"\"\n",
    "    for agent in agents_arr:\n",
    "        #Aqui tenemos el problema de no conocer sus starting positions originales??\n",
    "        #env = createNNEnv(shape, randomStart=True)\n",
    "        if agents_arr.index(agent) < NUM_DQN_AGENTS:\n",
    "            print(\"DQN Agent\", agents_arr.index(agent)+1,\"/\",len(agents_arr))\n",
    "            path_to_load = \"../data/agent_models/pytorch/DQNagent\"+str(agents_arr.index(agent)+1)+\".pt\"\n",
    "        else:\n",
    "            print(\"DDQN Agent\", agents_arr.index(agent)+1,\"/\",len(agents_arr))\n",
    "            path_to_load = \"../data/agent_models/pytorch/DDQNagent\"+str(agents_arr.index(agent)+1-NUM_DQN_AGENTS)+\".pt\"\n",
    "        agent.qnetwork_local.load_state_dict(torch.load(path_to_load))\n",
    "        agent.qnetwork_local.eval()\"\"\"\n",
    "\n",
    "#Demostracion de un agente en el entorno\n",
    "#Hay que tener cuidado de darle correctamente el mismo tamaño de capas que al modelo que se quiere cargar\n",
    "\"\"\"agent_replay = DDQNAgent(state_size=env.observation_space.shape[0],action_size=env.action_space.n,seed=0, fc1_unit=128, fc2_unit=128)\n",
    "agent_replay.qnetwork_local.load_state_dict(torch.load('../data/agent_models/pytorch/DDQNagent4.pt'))\n",
    "agent_replay.qnetwork_local.eval()\n",
    "\n",
    "with open(f\"../data/csv/historyDQN.csv\", 'a') as f:\n",
    "    #f.write(f\"step,y,x,action,next_y,next_x,reward,done\\n\")\n",
    "    for i in range(1):\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "        t = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent_replay.act(obs)\n",
    "            prev_state = [obs[0], obs[1], action]\n",
    "            obs, rew, done, _, _ = env.step(action)\n",
    "\n",
    "            #f.write(f\"{t},{prev_state[0]},{prev_state[1]},{prev_state[2]},{obs[0]},{obs[1]},{rew},{done}\\n\")\n",
    "            t += 1\n",
    "        print(\"Agente\", i+1, \"terminado en\", t, \"pasos\")\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
