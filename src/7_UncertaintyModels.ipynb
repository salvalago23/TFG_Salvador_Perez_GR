{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import json\n",
    "\n",
    "from utilities.jsonRW import *\n",
    "from utilities.plots import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5x5 --- DQN :  64\n",
      "5x5 --- DDQN :  65\n",
      "5x5 --- Q-Learning :  180\n",
      "14x14 --- DQN :  66\n",
      "14x14 --- DDQN :  38\n",
      "14x14 --- Q-Learning :  50\n"
     ]
    }
   ],
   "source": [
    "def procesar_json(algorithm, shape):\n",
    "    json_file_path = get_json_file_path(algorithm, shape)\n",
    "\n",
    "    # Load and parse the JSON data\n",
    "    with open(json_file_path, \"r\") as json_file:\n",
    "        agent_data = json.load(json_file)\n",
    "\n",
    "    families_dict = {}\n",
    "    count = 0\n",
    "\n",
    "    # Access the data for each agent entry\n",
    "    for entry in agent_data.get(\"agents_data\", []):\n",
    "        count += 1\n",
    "\n",
    "        id = entry[\"id\"]\n",
    "        algorithm = entry[\"algorithm\"]\n",
    "        shape = entry[\"shape\"]\n",
    "        episodes = entry[\"episodes\"]\n",
    "        max_steps = entry[\"max_steps\"]\n",
    "\n",
    "        starting_position = tuple(entry[\"starting_position\"])\n",
    "\n",
    "        string_policy_grid = np.array(entry[\"string_policy_grid\"])\n",
    "        value_grid = np.array(entry[\"value_grid\"])\n",
    "        policy_grid = np.array(entry[\"policy_grid\"])\n",
    "\n",
    "\n",
    "        sy_values = []\n",
    "        sx_values = []\n",
    "        a_values = []\n",
    "\n",
    "        for y in range(len(policy_grid)):\n",
    "            for x in range(len(policy_grid[0])):\n",
    "                #En sy y sx tengo las coordenadas de cada estado\n",
    "                sy_values.append(float(y))\n",
    "                sx_values.append(float(x))\n",
    "                a_values.append(float(policy_grid[y][x]))\n",
    "\n",
    "        \"\"\"#vamos a printear el resultado de los arrays\n",
    "        for i in range(len(sy_values)):\n",
    "            print(\"[\", sy_values[i], \",\",sx_values[i], \"] --> \", a_values[i])\n",
    "        \n",
    "        break\"\"\"\n",
    "\n",
    "        # Si la familia ya existe en el diccionario, simplemente agrega los valores\n",
    "        if starting_position in families_dict:\n",
    "            \"\"\"            families_dict[starting_position][\"sy_values\"].extend(sy_values)\n",
    "            families_dict[starting_position][\"sx_values\"].extend(sx_values)\n",
    "            families_dict[starting_position][\"a_values\"].extend(a_values)\"\"\"\n",
    "            families_dict[starting_position][\"sy_values\"].append(sy_values)\n",
    "            families_dict[starting_position][\"sx_values\"].append(sx_values)\n",
    "            families_dict[starting_position][\"a_values\"].append(a_values)\n",
    "        \n",
    "        else:\n",
    "            # Si la familia no existe, crea una nueva entrada en el diccionario\n",
    "            families_dict[starting_position] = {\n",
    "                \"sy_values\": [sy_values],\n",
    "                \"sx_values\": [sx_values],\n",
    "                \"a_values\": [a_values]\n",
    "            }\n",
    "\n",
    "    return families_dict, count\n",
    "\n",
    "\n",
    "algorithms = [\"DQN\", \"DDQN\", \"Q-Learning\"]\n",
    "shapes = [\"5x5\", \"14x14\"]\n",
    "\n",
    "for shape in shapes:\n",
    "    for algorithm in algorithms:\n",
    "        #train_inputs, train_labels = procesar_json(json_path)\n",
    "        families_dict, count = procesar_json(algorithm, shape)\n",
    "\n",
    "        totalY, totalX, totalA = 0, 0, 0\n",
    "        #print the length of each key\n",
    "        for key in families_dict:\n",
    "            totalY += len(families_dict[key][\"sy_values\"])\n",
    "            totalX += len(families_dict[key][\"sx_values\"])\n",
    "            totalA += len(families_dict[key][\"a_values\"])\n",
    "            #print(key)\n",
    "            #print(len(return_dict[key][\"sy_values\"]))\n",
    "            #print(len(return_dict[key][\"sx_values\"]))\n",
    "            #print(len(return_dict[key][\"a_values\"]))\n",
    "\n",
    "        if totalY == totalX and totalX == totalA and totalA == count:\n",
    "            print(shape, \"---\", algorithm, \": \", count)\n",
    "        else:\n",
    "            print(shape, \"---\", algorithm, \": ERROR\")\n",
    "            \n",
    "#print(return_dict[(7,0)][\"sy_values\"])\n",
    "\n",
    "# Ahora, train_inputs y train_labels contienen los datos en formato de tensores para entrenar tus modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tu arquitectura de red neuronal\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.softmax(self.fc(x), dim=-1)\n",
    "\n",
    "# Función para entrenar un modelo por familia de trayectorias\n",
    "def train_model(data_loader, input_size, output_size, num_epochs=10, lr=0.001):\n",
    "    model = PolicyNetwork(input_size, output_size)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, labels in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return model\n",
    "\n",
    "# Supongamos que tienes datos de entrenamiento en forma de torch tensors 'train_inputs' y 'train_labels'\n",
    "# Crea DataLoader para cada familia de trayectorias\n",
    "# train_inputs y train_labels deben contener tus datos de entrenamiento para cada familia\n",
    "\n",
    "# Ejemplo:\n",
    "train_inputs_family1, train_labels_family1 = torch.Tensor(...), torch.LongTensor(...)\n",
    "train_dataset_family1 = TensorDataset(train_inputs_family1, train_labels_family1)\n",
    "train_loader_family1 = DataLoader(train_dataset_family1, batch_size=64, shuffle=True)\n",
    "\n",
    "# Entrena un modelo para la familia de trayectorias 1\n",
    "input_size = ...  # Define el tamaño de entrada según tus datos\n",
    "output_size = ...  # Define el tamaño de salida según tus datos\n",
    "model_family1 = train_model(train_loader_family1, input_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para combinar modelos individuales en un modelo global\n",
    "def combine_models(models):\n",
    "    class CombinedModel(nn.Module):\n",
    "        def __init__(self, models):\n",
    "            super(CombinedModel, self).__init__()\n",
    "            self.models = nn.ModuleList(models)\n",
    "\n",
    "        def forward(self, x):\n",
    "            outputs = [model(x) for model in self.models]\n",
    "            return torch.mean(torch.stack(outputs), dim=0)\n",
    "\n",
    "    return CombinedModel(models)\n",
    "\n",
    "# Combina los modelos individuales en un modelo global\n",
    "global_model = combine_models([model_family1, model_family2, ...])  # Agrega todos los modelos necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para capturar la incertidumbre utilizando Monte Carlo\n",
    "def capture_uncertainty(model, state, num_samples=100):\n",
    "    outputs = torch.zeros((num_samples, state.size(0), model.fc.out_features))\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        outputs[i, :, :] = model(state)\n",
    "\n",
    "    uncertainty = torch.var(outputs, dim=0).mean(dim=-1)\n",
    "    return uncertainty\n",
    "\n",
    "# Supongamos que tienes un estado 'sample_state' para el cual deseas capturar la incertidumbre\n",
    "sample_state = torch.Tensor(...)  # Reemplaza ... con tu estado\n",
    "\n",
    "# Captura la incertidumbre para el estado dado utilizando Monte Carlo\n",
    "uncertainty = capture_uncertainty(global_model, sample_state)\n",
    "print(\"Uncertainty:\", uncertainty)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TFG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
