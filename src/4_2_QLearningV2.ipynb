{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import sys\n",
        "sys.path.append('../')\n",
        "\n",
        "from agentsClasses.QLearningClass import QLearningAgent\n",
        "from utilities.plots import create_grids, plot_trajectory\n",
        "from utilities.jsonRW import writeJSON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "#Hyperparameters\n",
        "export_to_JSON = False  # write JSON file\n",
        "\n",
        "NUM_AGENTS = 2\n",
        "\n",
        "SHAPE = \"5x5\"           # shape of the grid environment\n",
        "\n",
        "\"\"\"if SHAPE == \"5x5\":\n",
        "    EPISODES_PER_AGENT = 3000\n",
        "    MAX_STEPS_PER_EPISODE = 100\n",
        "elif SHAPE == \"14x14\":\n",
        "    EPISODES_PER_AGENT = 6000\n",
        "    MAX_STEPS_PER_EPISODE = 200\"\"\"\n",
        "\n",
        "episodes = [3000]\n",
        "steps = [100,150,200]\n",
        "\n",
        "EPS_START = 1.0\n",
        "#EPS_DECAY = EPS_START/(EPISODES_PER_AGENT/2)  # reduce the exploration over time\n",
        "EPS_END = 0.1\n",
        "\n",
        "DISCOUNT_FACTOR = 0.95\n",
        "LR = 0.02"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training with 3000 episodes and 100 steps per episode\n",
            "Starting training of 2 Q-learning agents\n",
            "Agent 1 / 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3000/3000 [00:11<00:00, 267.97it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent 2 / 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3000/3000 [00:11<00:00, 262.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training with 3000 episodes and 150 steps per episode\n",
            "Starting training of 2 Q-learning agents\n",
            "Agent 1 / 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3000/3000 [00:11<00:00, 263.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent 2 / 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3000/3000 [00:09<00:00, 321.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training with 3000 episodes and 200 steps per episode\n",
            "Starting training of 2 Q-learning agents\n",
            "Agent 1 / 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3000/3000 [00:09<00:00, 313.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Agent 2 / 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3000/3000 [00:11<00:00, 257.38it/s]\n"
          ]
        }
      ],
      "source": [
        "#CREATING AND TRAINING THE AGENTS\n",
        "agents_arr = []\n",
        "\n",
        "for n_episode in episodes:\n",
        "    EPISODES_PER_AGENT = n_episode\n",
        "    EPS_DECAY = EPS_START/(EPISODES_PER_AGENT/2)\n",
        "\n",
        "    for n_steps in steps:\n",
        "        MAX_STEPS_PER_EPISODE = n_steps\n",
        "\n",
        "        print(\"Starting training with\", EPISODES_PER_AGENT, \"episodes and\", MAX_STEPS_PER_EPISODE, \"steps per episode\")\n",
        "        print(\"Starting training of\", NUM_AGENTS, \"Q-learning agents\")\n",
        "\n",
        "        for i in range(NUM_AGENTS):\n",
        "            print(\"Agent\", i+1, \"/\", NUM_AGENTS)\n",
        "            agent = QLearningAgent(i, SHAPE, EPISODES_PER_AGENT, MAX_STEPS_PER_EPISODE, LR, EPS_START, EPS_DECAY, EPS_END, DISCOUNT_FACTOR)\n",
        "            \n",
        "            agent.train()\n",
        "            agents_arr.append(agent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "for agent in agents_arr:\n",
        "    value_grid, policy_grid, string_policy_grid = create_grids(agent.env, agent=agent)\n",
        "    start_pos = agent.env.unwrapped.start_pos\n",
        "\n",
        "    if export_to_JSON:\n",
        "        writeJSON(agent.algorithm, agent.n_episodes, agent.max_steps, agent.shape, start_pos, value_grid, policy_grid, string_policy_grid)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
