{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NNGridWorldEnv(gym.Env):\n",
        "    def __init__(self, maze, grid_model_path, reward_model_path):\n",
        "        self.maze = np.array(maze)\n",
        "        self.start_pos = (np.concatenate(np.where(self.maze == 'S'))).astype(np.int32)  # Starting position\n",
        "        self.goal_pos = (np.concatenate(np.where(self.maze == 'G'))).astype(np.int32)  # Goal position\n",
        "        self.num_rows, self.num_cols = self.maze.shape\n",
        "\n",
        "        self.observation_space = spaces.Box(low=np.array([0, 0]), high=np.array([self.num_rows, self.num_cols]), dtype=np.int32)\n",
        "        self.action_space = spaces.Discrete(4)\n",
        "\n",
        "        # Load models\n",
        "        print(\"Loading models...\")\n",
        "        self.grid_model = tf.keras.models.load_model(grid_model_path)\n",
        "        self.reward_model = tf.keras.models.load_model(reward_model_path)\n",
        "        print(\"Models loaded\")\n",
        "\n",
        "\n",
        "    def _is_valid_position(self, pos):\n",
        "        row, col = pos\n",
        "        # If agent hits a wall or goes out of the grid\n",
        "        if self.maze[row, col] == '#' or row < 0 or col < 0 or row >= self.num_rows or col >= self.num_cols:\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        self._agent_location = self.start_pos \n",
        "        self._target_location = self.goal_pos\n",
        "\n",
        "        return self._agent_location, {}\n",
        "    \n",
        "    def step(self, action):\n",
        "        input_model = np.column_stack(np.array([self._agent_location[0], self._agent_location[1], action]))\n",
        "\n",
        "        #round the values\n",
        "        new_pos = np.array(np.round(self.grid_model.predict(input_model, verbose=0)[0]), dtype=int)\n",
        "        reward = int(np.round(self.reward_model.predict(input_model, verbose=0)[0]))\n",
        "\n",
        "        # Check if the new position is valid\n",
        "        if self._is_valid_position(new_pos):\n",
        "            self._agent_location = new_pos\n",
        "\n",
        "        # An episode is done if the agent has reached the target\n",
        "        terminated = np.array_equal(self._agent_location, self._target_location)        \n",
        "\n",
        "        return self._agent_location, reward, terminated, False, {}\n",
        "    \n",
        "    def render(self):\n",
        "        for row in self.maze:\n",
        "            string = \"\"\n",
        "            for col in row:\n",
        "                string += col\n",
        "            print(string)\n",
        "\n",
        "    def close(self):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading models...\n",
            "Models loaded\n",
            "..#.G\n",
            "..#..\n",
            ".....\n",
            "..#..\n",
            "S.#..\n"
          ]
        }
      ],
      "source": [
        "maze = [\n",
        "    ['.', '.', '#', '.', 'G'],\n",
        "    ['.', '.', '#', '.', '.'],\n",
        "    ['.', '.', '.', '.', '.'],\n",
        "    ['.', '.', '#', '.', '.'],\n",
        "    ['S', '.', '#', '.', '.'],\n",
        "]\n",
        "\n",
        "grid_model_path = '../data/models/modelo_entorno.h5'\n",
        "reward_model_path = '../data/models/modelo_reward.h5'\n",
        "\n",
        "\n",
        "env = NNGridWorldEnv(maze=maze, grid_model_path=grid_model_path, reward_model_path=reward_model_path)\n",
        "\n",
        "# try draw the grid world\n",
        "obs = env.reset()\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": false
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow.contrib'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\el_sa\\Desktop\\proyectoNN_salva\\src\\DQN.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/el_sa/Desktop/proyectoNN_salva/src/DQN.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstable_baselines\u001b[39;00m \u001b[39mimport\u001b[39;00m DQN, PPO2, A2C, ACKTR \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/el_sa/Desktop/proyectoNN_salva/src/DQN.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Train the agent\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/el_sa/Desktop/proyectoNN_salva/src/DQN.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model \u001b[39m=\u001b[39m DQN(\u001b[39m'\u001b[39m\u001b[39mMlpPolicy\u001b[39m\u001b[39m'\u001b[39m, env, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mlearn(\u001b[39m100000\u001b[39m)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\stable_baselines\\__init__.py:7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstable_baselines\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39macer\u001b[39;00m \u001b[39mimport\u001b[39;00m ACER\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstable_baselines\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39macktr\u001b[39;00m \u001b[39mimport\u001b[39;00m ACKTR\n\u001b[1;32m----> 7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstable_baselines\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdeepq\u001b[39;00m \u001b[39mimport\u001b[39;00m DQN\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstable_baselines\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mher\u001b[39;00m \u001b[39mimport\u001b[39;00m HER\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstable_baselines\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mppo2\u001b[39;00m \u001b[39mimport\u001b[39;00m PPO2\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\stable_baselines\\deepq\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstable_baselines\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdeepq\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpolicies\u001b[39;00m \u001b[39mimport\u001b[39;00m MlpPolicy, CnnPolicy, LnMlpPolicy, LnCnnPolicy\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstable_baselines\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdeepq\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbuild_graph\u001b[39;00m \u001b[39mimport\u001b[39;00m build_act, build_train  \u001b[39m# noqa\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mstable_baselines\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdeepq\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdqn\u001b[39;00m \u001b[39mimport\u001b[39;00m DQN\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\stable_baselines\\deepq\\policies.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcontrib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlayers\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf_layers\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgym\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspaces\u001b[39;00m \u001b[39mimport\u001b[39;00m Discrete\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.contrib'"
          ]
        }
      ],
      "source": [
        "from stable_baselines import DQN, PPO2, A2C, ACKTR \n",
        "\n",
        "# Train the agent\n",
        "model = DQN('MlpPolicy', env, verbose=1).learn(100000)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
